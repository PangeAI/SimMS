{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates performance gains of `CudaCosineGreedy` against `matchms`.\n",
    "\n",
    "Performance depends heavily on used hardware, as well as how correlated the spectra are with each other, (i.e. on average, how many common pairs of peaks do spectra have). \n",
    "\n",
    "This specific notebook evaluates the performance given the following below arguments (feel free to change these):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4090 (UUID: GPU-377e4e40-ce36-d6dc-f5fe-c716f302a235)\n",
      "Number of CPU cores 64\n"
     ]
    }
   ],
   "source": [
    "# We can use pickled version for much faster loading times\n",
    "spectra_file = 'GNPS-LIBRARY-default-filter-nmax-2048.pickle'\n",
    "\n",
    "# We take a random sample of spectra from said file\n",
    "\n",
    "# Minimum size:\n",
    "chunk_sizes_min = 32\n",
    "\n",
    "# Maximum size\n",
    "chunk_sizes_max = 10_000\n",
    "\n",
    "# how many points to evaluate (in logspace) between min and max\n",
    "num_evals = 15\n",
    "\n",
    "# max number of peaks to retain in any spectra - larger numbers are marginally more accurate, but much slower\n",
    "n_max_peaks = 1024\n",
    "\n",
    "# Match limit\n",
    "match_limit = 2048\n",
    "\n",
    "# tolerance\n",
    "tolerance = 0.1\n",
    "\n",
    "# optimal batch size is hardware-dependent, but usually the best number is the largest the hardware can handle (without an OOM error)\n",
    "batch_size = 4096\n",
    "\n",
    "# Hardware matters! These results are only repeatable using this GPU (shown as an output)\n",
    "! nvidia-smi -L\n",
    "! echo Number of CPU cores $(nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip uninstall cudams -q -y\n",
    "! pip install -q --upgrade git+https://github.com/tornikeo/cudams@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sparsestack/StackedSparseArray.py:4: DeprecationWarning: Please use `get_index_dtype` from the `scipy.sparse` namespace, the `scipy.sparse.sputils` namespace is deprecated.\n",
      "  from scipy.sparse.sputils import get_index_dtype\n"
     ]
    }
   ],
   "source": [
    "from cudams.utils import argbatch, Timer\n",
    "from cudams.similarity.spectrum_similarity_functions import cosine_greedy_kernel\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/cudams/utils.py:267: UserWarning: As of 2024, ALL_GNPS.pickle is a large file (1.76GB) make sure the machine can handle this\n",
      "  warnings.warn(\n",
      "Downloading data from 'https://github.com/tornikeo/cudams/releases/download/samples-0.1/ALL_GNPS.pickle' to file '/root/.cache/pooch/a77e59fe0d8571b2d91a56e12790b36a-ALL_GNPS.pickle'.\n",
      "100%|██████████████████████████████████████| 1.47G/1.47G [00:00<00:00, 849GB/s]\n",
      "SHA256 hash of downloaded file: 473593b86b35de9bc90c216b70ca7f64628bf577cb4bc64afd1d4d0a533ab1f9\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "/tmp/ipykernel_383/2885043538.py:8: ResourceWarning: unclosed file <_io.BufferedReader name='/root/.cache/pooch/a77e59fe0d8571b2d91a56e12790b36a-ALL_GNPS.pickle'>\n",
      "  spectra = pickle.load(open(download(spectra_file),'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cudams.utils import download\n",
    "from joblib import Parallel, delayed\n",
    "from matchms.filtering import default_filters, normalize_intensities, reduce_to_number_of_peaks\n",
    "from matchms.importing import load_from_mgf\n",
    "from cudams.utils import mute_stdout\n",
    "\n",
    "spectra = pickle.load(open(download(spectra_file),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:05<00:00,  8.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from cudams.similarity import CudaCosineGreedy\n",
    "\n",
    "np.random.seed(42)\n",
    "chunk_sizes_cu = np.round(np.logspace(\n",
    "    np.log2(chunk_sizes_min), \n",
    "    np.log2(chunk_sizes_max), \n",
    "    num=num_evals, \n",
    "    base=2, \n",
    "    endpoint=True)\n",
    ").astype(int)\n",
    "\n",
    "times_cu = []\n",
    "pairs_cu = []\n",
    "\n",
    "# Kernel compilation might take a bit of time initially\n",
    "kernel = CudaCosineGreedy(batch_size=batch_size, \n",
    "                          n_max_peaks=n_max_peaks)\n",
    "\n",
    "# To force CUDA to load code to GPU, we need to do warmup\n",
    "kernel.matrix(spectra[:64], spectra[:64])\n",
    "\n",
    "# We avoid parallel here, since we only have one GPU, after all.\n",
    "for chunk_size in tqdm(chunk_sizes_cu):\n",
    "    chunk_size = min(len(spectra), chunk_size) # We might run out of spectra\n",
    "    references = spectra[:chunk_size]\n",
    "    queries = references # Pairwise\n",
    "    with Timer() as timer:\n",
    "        kernel.matrix(references, queries)\n",
    "    times_cu.append(timer.duration)\n",
    "    pairs_cu.append(len(references) * len(queries)) # We've processed all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"pairs_cu\": [\n",
      "  1024,\n",
      "  2500,\n",
      "  6084,\n",
      "  14884,\n",
      "  36100,\n",
      "  88209,\n",
      "  215296,\n",
      "  524176,\n",
      "  1279161,\n",
      "  3115225,\n",
      "  7595536,\n",
      "  18524416,\n",
      "  45158400,\n",
      "  110103049,\n",
      "  268435456\n",
      " ],\n",
      " \"times_cu\": [\n",
      "  0.7942757579730824,\n",
      "  0.8001212710514665,\n",
      "  0.8135092230513692,\n",
      "  0.8842686890857294,\n",
      "  0.8815657449886203,\n",
      "  0.9123411850305274,\n",
      "  0.8964556299615651,\n",
      "  0.9994770409539342,\n",
      "  1.1101172500057146,\n",
      "  1.4267874190118164,\n",
      "  2.239893207908608,\n",
      "  7.677725055953488,\n",
      "  13.924317113007419,\n",
      "  26.042265221010894,\n",
      "  65.73885893099941\n",
      " ],\n",
      " \"device\": \"NVIDIA GeForce RTX 4090\",\n",
      " \"nproc\": 255\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "benchmark_summary = dict(\n",
    "        pairs_cu=pairs_cu,\n",
    "        times_cu=times_cu,\n",
    "        device=torch.cuda.get_device_name(),\n",
    "        nproc=os.cpu_count(),    \n",
    "    )\n",
    "print(json.dumps(benchmark_summary,indent=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

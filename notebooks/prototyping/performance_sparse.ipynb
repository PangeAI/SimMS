{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cudams.utils import argbatch, mkdir\n",
    "from cudams.data import get_ref_spectra_from_df\n",
    "from cudams.kernel import compile\n",
    "from cudams.utils import name2idx\n",
    "from cudams.cosine import similarity\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from cudams.data import spectra_peaks_to_tensor\n",
    "from cudams.processor import Config\n",
    "from numba import cuda\n",
    "from itertools import product\n",
    "from time import perf_counter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import shared_memory\n",
    "import numpy as np\n",
    "import json\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "assert cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "tolerance: float = 0.1\n",
    "shift: float = 0\n",
    "mz_power: float = 0\n",
    "int_power: float = 1\n",
    "\n",
    "## How many pairs per batch. Has to be a power of 2.\n",
    "# Hardware specific - An RTX2070 works best at around 1024 * 2\n",
    "# But Colab T4 GPU might work best at 1024 * 4\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "# MAX NUMBER OF PEAKS during filtering. Due to nature of matrices, having large number of \n",
    "# peaks will increase memory requirements. After 1024, this has diminishing benefits, as \n",
    "# smaller and smaller (likely noisy) peaks are taken into consideration when running similarity.\n",
    "MAX_PEAKS = 1024\n",
    "\n",
    "# MATCH_LIMIT specifies max how many mz-mz pairs we could consider for each RQ pair, before we sort and filter. \n",
    "# E.g. a value of 256 usually causes around ~0.003% of RQ pairs to \"overflow\".\n",
    "# The overflown RQ scores will be strictly less than or equal to perfectly accurate score.\n",
    "# The mean absolute difference at 256, for all overflown pairs is on the order of ~1e-3\n",
    "# Small values of MATCH_LIMIT (e.g. 128, 64,) cause a dramatic speedup in the processing speed.\n",
    "MATCH_LIMIT = 1024\n",
    "\n",
    "# Since Greedy cosine is an unstable algorithm, because approximate mz-mz values do not\n",
    "# result in approximately the same scores and number of matches.\n",
    "# So we need to use fp64 to minimize the deviation as much as possible.\n",
    "# Using float32 causes a significant speedup in the processing speed.\n",
    "dtype = 'float32'\n",
    "\n",
    "# Data path\n",
    "reference_csv_file = Path(\"data/input/example_dataset_tornike.csv\")\n",
    "query_csv_file = Path(\"data/input/example_dataset_tornike.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100001/100001 [00:34<00:00, 2877.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from cudams.processor import CudaCosineGreedy, CpuCosineGreedy\n",
    "from collections import defaultdict\n",
    "from matchms import calculate_scores\n",
    "from matchms.similarity import CosineGreedy\n",
    "from tqdm import tqdm\n",
    "from matchms.filtering import normalize_intensities, select_by_mz, select_by_relative_intensity, reduce_to_number_of_peaks, \\\n",
    "    require_minimum_number_of_peaks\n",
    "from cudams.utils import mute_stdout\n",
    "\n",
    "def process_spectrum(spectrum: np.ndarray) -> np.ndarray:\n",
    "    spectrum = select_by_mz(spectrum, mz_from=10.0, mz_to=1000.0)\n",
    "    spectrum = normalize_intensities(spectrum)\n",
    "    spectrum = select_by_relative_intensity(spectrum, intensity_from=0.001)\n",
    "    spectrum = reduce_to_number_of_peaks(spectrum, n_max=MAX_PEAKS)\n",
    "    spectrum = require_minimum_number_of_peaks(spectrum, n_required=5)\n",
    "    return spectrum\n",
    "\n",
    "ref_spectra_df_path = Path(reference_csv_file)\n",
    "ref_spectra_df = pd.read_csv(ref_spectra_df_path)\n",
    "references = get_ref_spectra_from_df(ref_spectra_df, spectrum_processor=process_spectrum)\n",
    "\n",
    "# query_spectra_df_path = Path(query_csv_file)\n",
    "# query_spectra_df = pd.read_csv(query_spectra_df_path)\n",
    "# queries = get_ref_spectra_from_df(query_spectra_df, spectrum_processor=process_spectrum)\n",
    "queries = references[:]\n",
    "\n",
    "\n",
    "## awkward arrays pip package can handle uneven arrays pretty well. We can use that to\n",
    "# refs = ak.from_parquet('data/input/example_dataset_tornike.parquet')\n",
    "# ques = ak.from_parquet('data/input/example_dataset_tornike.parquet')\n",
    "# data_uneven = ak.from_iter([r.peaks.to_numpy for r in data])\n",
    "# data_uneven = ak.values_astype(data_uneven, 'float32')\n",
    "# ak.to_parquet(data_uneven, 'data/input/example_dataset_tornike.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 46it [00:02, 22.98it/s]\n"
     ]
    }
   ],
   "source": [
    "batches_r = []\n",
    "for bstart, bend in tqdm(\n",
    "    argbatch(references, BATCH_SIZE), desc=\"Batch all references\"\n",
    "):\n",
    "    rbatch = references[bstart:bend]\n",
    "    rspec, rlen = spectra_peaks_to_tensor(rbatch, dtype=dtype)\n",
    "    batches_r.append([rspec, rlen, bstart, bend])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all queries: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all queries: 46it [00:01, 24.50it/s]\n"
     ]
    }
   ],
   "source": [
    "batches_q = []\n",
    "for bstart, bend in tqdm(\n",
    "    argbatch(queries, BATCH_SIZE), desc=\"Batch all queries\"\n",
    "):\n",
    "    qbatch = queries[bstart:bend]\n",
    "    qspec, qlen = spectra_peaks_to_tensor(qbatch, dtype=dtype)\n",
    "    batches_q.append([qspec, qlen, bstart, bend])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_rq = list(product(batches_r, batches_q))\n",
    "\n",
    "TOTAL_BATCHES = len(batches_rq)\n",
    "\n",
    "batch_outputs = np.empty(shape=(TOTAL_BATCHES,4),dtype=object)\n",
    "streams = [cuda.stream() for _ in range(TOTAL_BATCHES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cudams.kernel import compile\n",
    "\n",
    "kernel = compile(\n",
    "    tolerance=tolerance,\n",
    "    shift=shift,\n",
    "    mz_power=mz_power,\n",
    "    int_power=int_power,\n",
    "    match_limit=MATCH_LIMIT,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def end_of_stream_callback(\n",
    "        stream, \n",
    "        status, \n",
    "        rstart,\n",
    "        rend,\n",
    "        qstart,\n",
    "        qend):\n",
    "    pass\n",
    "    # We order a data return\n",
    "    \n",
    "    # out = out_cu.copy_to_host(stream=stream)\n",
    "    # overflow = overflow_cu.copy_to_host(stream=stream)\n",
    "    # lens = lens_cu.copy_to_host(stream=stream)\n",
    "    \n",
    "    # mask = out[:len(rlen),:len(qlen),0] >= threshold\n",
    "    # # r, c = np.nonzero(mask)\n",
    "    # out = out[r,c]\n",
    "    # overflow = overflow[r,c]\n",
    "    # r += rstart\n",
    "    # c += qstart\n",
    "    # batch_outputs[batch_i] = r, c, out, overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# We loop over all batchs in sequence\n",
    "out = np.empty(\n",
    "    shape=(BATCH_SIZE, BATCH_SIZE, 2),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "overflow = np.empty(\n",
    "    shape=(BATCH_SIZE, BATCH_SIZE, 1),\n",
    "    dtype=\"uint8\",\n",
    ")\n",
    "for batch_i in tqdm(range(30)):\n",
    "    stream = cuda.stream()\n",
    "    # stream = cuda.default_stream()\n",
    "    # Each batch has own CUDA stream so that the GPU is as busy as possible\n",
    "\n",
    "    # We get our batch and lengths (lengths are different for different spectra)\n",
    "    (rspec, rlen, rstart, rend), (qspec, qlen, qstart, qend) = batches_rq[\n",
    "        batch_i\n",
    "    ]\n",
    "    lens = np.zeros((2, BATCH_SIZE), \"int32\")\n",
    "    lens[0, : len(rlen)] = rlen\n",
    "    lens[1, : len(qlen)] = qlen\n",
    "\n",
    "    # We make sure main resources remain on CPU RAM\n",
    "    with cuda.pinned(\n",
    "        rspec,\n",
    "        qspec,\n",
    "        lens,\n",
    "        out,\n",
    "        overflow,\n",
    "    ):\n",
    "        # We order empty space for results on GPU RAM\n",
    "        out_cu = cuda.device_array(\n",
    "            (BATCH_SIZE, BATCH_SIZE, 2), dtype=\"float32\",\n",
    "            stream=stream\n",
    "        )\n",
    "        overflow_cu = cuda.device_array(\n",
    "            (BATCH_SIZE, BATCH_SIZE, 1), dtype=\"uint8\",\n",
    "            stream=stream\n",
    "        )\n",
    "\n",
    "        # We order the stream to copy input data to GPU RAM\n",
    "        rspec_cu = cuda.to_device(rspec, stream=stream)\n",
    "        qspec_cu = cuda.to_device(qspec, stream=stream)\n",
    "        lens_cu = cuda.to_device(lens, stream=stream)\n",
    "\n",
    "        # We order the stream to execute kernel (this is scheduled, it will execute, but we can't force it)\n",
    "        kernel(\n",
    "            rspec_cu, qspec_cu, lens_cu, out_cu, overflow_cu,\n",
    "            stream=stream\n",
    "        )\n",
    "        # result_output[rstart:rend, qstart:qend] = out\n",
    "        # result_overflow[rstart:rend, qstart:qend] = overflow\n",
    "        def cb(stream, status, rstart, rend, qstart, qend):\n",
    "            print(stream, status, rstart, rend, qstart, qend)\n",
    "            \n",
    "        stream.add_callback(\n",
    "            callback=cb,\n",
    "            arg=[\n",
    "                rstart,\n",
    "                rend,\n",
    "                qstart,\n",
    "                qend,\n",
    "            ],\n",
    "        )\n",
    "# We wait for all streams to finish their work everywhere\n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 46it [00:01, 27.33it/s]\n",
      "Batch all queries: 46it [00:01, 27.86it/s]\n",
      "  0%|          | 0/2116 [00:00<?, ?it/s]/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:2290: UserWarning: Exception in stream callback: CudaCosineGreedy._matrix_with_sparse_output.<locals>.end_of_stream_callback() missing 3 required positional arguments: 'rend', 'qstart', and 'qend'\n",
      "  warnings.warn(f\"Exception in stream callback: {e}\")\n",
      "  3%|▎         | 61/2116 [00:21<14:36,  2.34it/s] Exception ignored in: <finalize object at 0x7f64e452c640; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/weakref.py\", line 591, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 1660, in core\n",
      "    driver.cuMemHostUnregister(ptr)\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 326, in safe_cuda_api_call\n",
      "    retcode = libfn(*args)\n",
      "KeyboardInterrupt: \n",
      "  7%|▋         | 157/2116 [00:57<12:55,  2.53it/s]Exception ignored in: <finalize object at 0x7f64f0086da0; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/weakref.py\", line 591, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 1660, in core\n",
      "    driver.cuMemHostUnregister(ptr)\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 326, in safe_cuda_api_call\n",
      "    retcode = libfn(*args)\n",
      "KeyboardInterrupt: \n",
      "  9%|▉         | 201/2116 [01:09<09:07,  3.50it/s]"
     ]
    }
   ],
   "source": [
    "from cudams.processor import CudaCosineGreedy\n",
    "from tqdm import tqdm\n",
    "\n",
    "rlims = argbatch(refs, BATCH_SIZE)\n",
    "qlims = argbatch(ques, BATCH_SIZE)\n",
    "\n",
    "R = len(references)\n",
    "Q = len(queries)\n",
    "\n",
    "batches_rq = list(product(rlims, qlims))\n",
    "\n",
    "cosine = CudaCosineGreedy(\n",
    "    tolerance=tolerance,\n",
    "    mz_power=0,\n",
    "    intensity_power=1, \n",
    "    shift=0,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    match_limit=MATCH_LIMIT,\n",
    ")\n",
    "cosine.compile()\n",
    "t = perf_counter()\n",
    "ri, qi, out, overflows = cosine.matrix(\n",
    "    references=references, \n",
    "    queries=queries, \n",
    "    array_type=\"sparse\",\n",
    "    sparse_threshold=.75,\n",
    ")\n",
    "t = perf_counter() - t\n",
    "sum_nbytes = sum(o.nbytes for o in [ri, qi, out, overflows])\n",
    "print(f\"Output size {sum_nbytes / 1e9:.2f}GB\")\n",
    "print(f\"Num of output {len(ri)}\")\n",
    "print(f\"Pairs processed {len(references) * len(queries):.1e}\")\n",
    "n_pairs = len(references) * len(queries)\n",
    "perh = (n_pairs / t) * 3600\n",
    "\n",
    "print(f\"pairs per hr {perh:.1e}\")\n",
    "print(f\"Full run (100kx1.5mln) est: {100_000 * 1_500_000 / perh:.3f}hrs\")\n",
    "print(f\"Full run (100kx1.5mln) est GBs: {(sum_nbytes/n_pairs)*(100_000*1_500_000)*1e-9:.2f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

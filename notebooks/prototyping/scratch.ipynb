{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "from multiprocessing import shared_memory\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tqdm import tqdm\n",
    "import numba\n",
    "from typing import Tuple, List, Optional\n",
    "from matchms import Spectrum\n",
    "from matchms.typing import SpectrumType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from numba import cuda\n",
    "from numba.cuda.cudadrv.devicearray import DeviceNDArray\n",
    "from numba import types\n",
    "import math\n",
    "import warnings\n",
    "from numba.core.errors import NumbaPerformanceWarning\n",
    "import time\n",
    "from time import perf_counter\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from matchms.filtering import normalize_intensities\n",
    "from matchms.filtering import require_minimum_number_of_peaks\n",
    "from matchms.filtering import select_by_mz\n",
    "from matchms.filtering import select_by_relative_intensity\n",
    "from matchms.filtering import reduce_to_number_of_peaks\n",
    "from matchms.filtering import add_losses\n",
    "\n",
    "assert cuda.is_available()\n",
    "from cudams.utils import ignore_performance_warnings\n",
    "ignore_performance_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, int32\n",
    "\n",
    "BSP2 = 9\n",
    "BLOCK_SIZE = 2**BSP2\n",
    "\n",
    "#CUDA kernel to calculate prefix sum of each block of input array\n",
    "@cuda.jit('void(int32[:], int32[:], int32[:], int32, int32)')\n",
    "def prefix_sum_nzmask_block(a, b, s, nzm, length):\n",
    "    ab = cuda.shared.array(shape=(BLOCK_SIZE), dtype=int32)\n",
    "\n",
    "    tid = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x;\n",
    "    ab[cuda.threadIdx.x] = 0\n",
    "    if tid < length:\n",
    "        if nzm == 1:\n",
    "            ab[cuda.threadIdx.x] = int32(a[tid] != 0); #Load mask of input data into shared memory\n",
    "        else:\n",
    "            ab[cuda.threadIdx.x] = int32(a[tid]); #Load input data into shared memory\n",
    "\n",
    "\n",
    "    for j in range(0,BSP2):\n",
    "        i = 2**j\n",
    "        cuda.syncthreads()\n",
    "        if i <= cuda.threadIdx.x:\n",
    "            temp = ab[cuda.threadIdx.x]\n",
    "            temp += ab[cuda.threadIdx.x - i] #Perform scan on shared memory\n",
    "        cuda.syncthreads()\n",
    "        if i <= cuda.threadIdx.x:\n",
    "            ab[cuda.threadIdx.x] = temp\n",
    "    if tid < length:\n",
    "        b[tid] = ab[cuda.threadIdx.x]; #Write scanned blocks to global memory\n",
    "\n",
    "    if(cuda.threadIdx.x == cuda.blockDim.x-1):  #Last thread of block\n",
    "        s[cuda.blockIdx.x] = ab[cuda.threadIdx.x]; #Write last element of shared memory into global memory\n",
    "\n",
    "#CUDA kernel to merge the prefix sums of individual blocks\n",
    "@cuda.jit('void(int32[:], int32[:], int32)')\n",
    "def pref_sum_update(b, s, length):\n",
    "    tid = (cuda.blockIdx.x + 1) * cuda.blockDim.x + cuda.threadIdx.x; #Skip first block\n",
    "\n",
    "    if tid<length:\n",
    "        b[tid] += s[cuda.blockIdx.x] #Accumulate last elements of all previous blocks\n",
    "\n",
    "\n",
    "#CUDA kernel to copy non-zero entries to the correct index of the output array\n",
    "@cuda.jit('void(int32[:], int32[:], int32[:], int32)')\n",
    "def map_non_zeros(a, prefix_sum, nz, length):\n",
    "    tid = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x;\n",
    "\n",
    "    if tid < length:\n",
    "        input_value = a[tid]\n",
    "        if input_value != 0:\n",
    "            index = prefix_sum[tid] #The correct output index is the value at current index of prefix sum array\n",
    "            nz[index-1] = input_value\n",
    "\n",
    "\n",
    "\n",
    "#Apply stream compaction algorithm to get only the non-zero entries from the input array\n",
    "def pref_sum(a, asum, nzm):\n",
    "    block = BLOCK_SIZE\n",
    "    length = a.shape[0]\n",
    "    grid = int((length + block -1)/block)\n",
    "    #Create auxiliary array to hold the sum of each block\n",
    "    bs = cuda.device_array(shape=(grid), dtype=np.int32)\n",
    "\n",
    "    #Perform partial scan of each block. Store block sum in auxiliary array named block_sum.\n",
    "    prefix_sum_nzmask_block[grid, block](a, asum, bs, nzm, length)\n",
    "    if grid > 1:\n",
    "        bssum = cuda.device_array(shape=(grid), dtype=np.int32)\n",
    "        pref_sum(bs, bssum, 0)\n",
    "        pref_sum_update[grid-1, block](asum, bssum, length)\n",
    "\n",
    "def get_non_zeros(a):\n",
    "    #Copy input array from host to device\n",
    "    ad = cuda.to_device(a)\n",
    "\n",
    "    #Create prefix sum output array\n",
    "    bd = cuda.device_array_like(ad)\n",
    "\n",
    "    #Perform partial scan of each block. Store block sum in auxiliary array named block_sum.\n",
    "    pref_sum(ad, bd, int(1))\n",
    "\n",
    "    #The last element of prefix sum contains the total number of non-zero elements\n",
    "    non_zero_count = int(bd[bd.shape[0]-1])\n",
    "    #Create device output array to hold ONLY the non-zero entries\n",
    "    non_zeros = cuda.device_array(shape=(non_zero_count), dtype=np.int32)\n",
    "\n",
    "    #Copy ONLY the non-zero entries\n",
    "    block = BLOCK_SIZE\n",
    "    length = a.shape[0]\n",
    "    grid = int((length + block -1)/block)\n",
    "    map_non_zeros[grid, block](ad, bd, non_zeros, length)\n",
    "\n",
    "    #Return to host\n",
    "    return non_zeros.copy_to_host()\n",
    "\n",
    "# arr = np.zeros(50000000, dtype=np.int32)\n",
    "# for i in range(32,65000, 1024):\n",
    "#     arr[i] = i\n",
    "# nz = get_non_zeros(arr)\n",
    "# print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000e+00 1.000e+00 2.000e+00 ... 1.021e+03 1.022e+03 1.023e+03]\n"
     ]
    }
   ],
   "source": [
    "# arr = np.arange(1024 * 1024).reshape(1024,1024).astype('float32')\n",
    "arr = np.arange(1024).reshape(1024).astype('float32')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "arr = np.random.uniform(size=(batch_size, batch_size)).astype('float32')\n",
    "threads_per_block = 32,32\n",
    "blocks_per_grid = (arr + threads_per_block[0] - 1) // threads_per_block[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,q = np.nonzero(arr > .7)\n",
    "v = arr[r,q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 1021 1022 1023]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(523776, 523776)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(1024).astype('int32')\n",
    "print(arr)\n",
    "\n",
    "\n",
    "THREADS_PER_BLOCK = 8\n",
    "# BLOCKS_PER_GRID_X = math.ceil(arr.shape[0] / THREADS_PER_BLOCK[0])\n",
    "# BLOCKS_PER_GRID_Y = math.ceil(arr.shape[1] / THREADS_PER_BLOCK[1])\n",
    "# BLOCKS_PER_GRID = BLOCKS_PER_GRID_X,BLOCKS_PER_GRID_Y\n",
    "BLOCKS_PER_GRID = (arr.shape[0] + THREADS_PER_BLOCK - 1) // THREADS_PER_BLOCK\n",
    "nelem = len(arr)\n",
    "\n",
    "@cuda.jit\n",
    "def array_sum(data):\n",
    "    tid = cuda.threadIdx.x\n",
    "    size = len(data)\n",
    "    if tid < size:\n",
    "        i = cuda.grid(1)\n",
    "\n",
    "        # Declare an array in shared memory\n",
    "        shr = cuda.shared.array(nelem, int32)\n",
    "        shr[tid] = data[i]\n",
    "\n",
    "        # Ensure writes to shared memory are visible\n",
    "        # to all threads before reducing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        s = 1\n",
    "        while s < cuda.blockDim.x:\n",
    "            if tid % (2 * s) == 0:\n",
    "                # Stride by `s` and add\n",
    "                shr[tid] += shr[tid + s]\n",
    "            s *= 2\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        # After the loop, the zeroth  element contains the sum\n",
    "        # if tid == 0:\n",
    "        data[i] = shr[tid]\n",
    "\n",
    "arr_cu = cuda.to_device(arr)\n",
    "out_cu = cuda.device_array_like(arr)\n",
    "array_sum[1, nelem](\n",
    "    arr_cu\n",
    ")\n",
    "out = arr_cu.copy_to_host()\n",
    "out[0], arr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8128,    1,    5, ..., 1021, 2045, 1023], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

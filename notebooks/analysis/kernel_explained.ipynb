{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what we are doing here, let's take a look at this image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](https://github.com/PangeAI/simms/blob/main/assets/cosine-batch-layout-grid.jpg?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1.5 million references and 100k arrays and want a stupidly large matrix of scores, with 1.5 million rows and 100k columns, where each matrix entry is a result of pairwise GreedyCosine. All entries are independent and can be computed in parallel. Even with high-CPU count (my machine has 8 CPU, estimate it takes 200 hours\n",
    "\n",
    "GPUs are fundamentally a large 2D grid of very small CPUs. There are several ways of making our problem \"fit\" to the environment of GPUs, and I have chosen the following layout as shown above.\n",
    "\n",
    "GPU can processes a single batch at a time - per-batch processing speed is near-instatanous, regardless of batch size, as long as the batch can fit into memory.\n",
    "\n",
    "So - every batch is a 2D grid of references and queries that will be compared pairwise by different threads. If we zoom into the batch#0, we see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](https://github.com/PangeAI/simms/blob/main/assets/cosine-batch-layout-batch.jpg?raw=true \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning that a GPU has a separate small CPU (thread) for every pair in the cartesian product of references and queries in that batch. We see that every thread takes in it's own reference and query and returns three values:\n",
    "score (float), num_matches (int, but casted to float), overflow (bool).\n",
    "\n",
    "If we further zoom into the first thread, we see this pseudo-code being executed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](https://github.com/PangeAI/simms/blob/main/assets/cosine-batch-layout-thread.jpg?raw=true \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is what is called a CUDA kernel - and it is exactly the same for every single thread in all batches. What changes is the input data (per batch) and which reference and query we work with (per thread).\n",
    "\n",
    "The algorithm has two parts.\n",
    "\n",
    "First loop collects all possible mzmz pairs (up to MATCH_LIMIT size), and report an overflow if it happens.\n",
    "\n",
    "Second loop is essentially a bubble sort. Since \"sorted()\" isn't available to CUDA threads, we have to manually loop over the matches (nested loop) and, while we have left over scores:\n",
    "- Get largest score\n",
    "- Discard all other scores that have same index\n",
    "- We normalize the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Download Data, Define Kernel, Data IO, Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in colab. Skipping install.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    print(\"Running in colab. Installing required libraries.\")\n",
    "    ! pip install -qq matchms numba joblib\n",
    "    ! mkdir -p data/input\n",
    "    ! cd data/input && wget -q https://storage.googleapis.com/PangeAI-tmp-io/example_dataset_tornike.csv\n",
    "except ImportError:\n",
    "    print(\"Not running in colab. Skipping install.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import numba\n",
    "from numba import cuda, types\n",
    "from numba.cuda.cudadrv.devicearray import DeviceNDArray\n",
    "\n",
    "\n",
    "def compile(\n",
    "    tolerance: float = 0.1,\n",
    "    shift: float = 0,\n",
    "    mz_power: float = 0.0,\n",
    "    int_power: float = 1.0,\n",
    "    match_limit: int = 128,\n",
    "    batch_size: int = 4096,\n",
    ") -> callable:\n",
    "    \"\"\"\n",
    "    JIT compiles the kernel for CUDA device, and bakes in constants (tolerance, shift, etc.)\n",
    "    Returns a callable that takes in arguments:\n",
    "        rspec_cu:\n",
    "            DeviceNDArray, [2, R, M] float32\n",
    "        qspec_cu:\n",
    "            DeviceNDArray, [2, Q, N] float32\n",
    "        lens_cu: DeviceNDArray, [2, max(R,Q)] int32\n",
    "            The \"2\" in front is because these is mz and int stacked on top of each other\n",
    "        out_cu: DeviceNDArray, [R,Q,2] float32\n",
    "            Contains both score (0) and counts (1) for each RQ pair\n",
    "        overflow_cu: DeviceNDArray, [R,Q,1] uint8\n",
    "            Contains 1 - if overflow happened at RQ\n",
    "        stream: cuda.stream\n",
    "            Necessary to keep GPU as busy as possible.\n",
    "\n",
    "    This callable will run JIT-ed cuda kernel. All arguments must already reside in GPU memory.\n",
    "    First-time use will cause the kernel \"warm-up\", so subsequent runs will be much faster.\n",
    "    \"\"\"\n",
    "    assert cuda.detect(), \"Cuda seems to be unavailable\"\n",
    "    MATCH_LIMIT = match_limit\n",
    "    R, Q = batch_size, batch_size\n",
    "    THREADS_PER_BLOCK = (32, 32)\n",
    "    BLOCKS_PER_GRID_X = math.ceil(R / THREADS_PER_BLOCK[0])\n",
    "    BLOCKS_PER_GRID_Y = math.ceil(Q / THREADS_PER_BLOCK[1])\n",
    "    BLOCKS_PER_GRID = (BLOCKS_PER_GRID_X, BLOCKS_PER_GRID_Y)\n",
    "\n",
    "    @cuda.jit\n",
    "    def _kernel(\n",
    "        rspec: DeviceNDArray,\n",
    "        qspec: DeviceNDArray,\n",
    "        lens: DeviceNDArray,\n",
    "        out: DeviceNDArray,\n",
    "        overflow: DeviceNDArray,\n",
    "    ):\n",
    "        i, j = cuda.grid(2)\n",
    "        thread_i = cuda.threadIdx.x\n",
    "        thread_j = cuda.threadIdx.y\n",
    "        block_size_x = cuda.blockDim.x\n",
    "        block_size_y = cuda.blockDim.y\n",
    "\n",
    "        # mem = cuda.shared.array((8, ))\n",
    "        # We aren't out of the RxQ grid\n",
    "        if i < R and j < Q:\n",
    "            # Init values (we expect these to be uninitialized)\n",
    "            overflow[i, j] = 0\n",
    "            out[i, j] = 0\n",
    "\n",
    "            # mem = cuda.shared.array((4, 4, 4, 32), types.float32)\n",
    "            rmz = rspec[0]\n",
    "            rint = rspec[1]\n",
    "            qmz = qspec[0]\n",
    "            qint = qspec[1]\n",
    "            # In this i,j, We get length of r and q spectrums\n",
    "            # since they are batched, there might be extra filler elements\n",
    "            rlen = lens[0]\n",
    "            qlen = lens[1]\n",
    "\n",
    "            rleni = rlen[i]\n",
    "            qlenj = qlen[j]\n",
    "\n",
    "            # When we have batch that is incomplete (size is indivisible by B)\n",
    "            # we return quickly to avoid writing garbage there.\n",
    "            if rleni == 0 or qlenj == 0:\n",
    "                return\n",
    "\n",
    "            spec1_mz = rmz[i]\n",
    "            spec1_int = rint[i]\n",
    "\n",
    "            spec2_mz = qmz[j]\n",
    "            spec2_int = qint[j]\n",
    "\n",
    "            lowest_idx = types.int32(0)\n",
    "            num_match = types.int32(0)\n",
    "\n",
    "            matches = cuda.local.array((2, MATCH_LIMIT), types.int16)\n",
    "            for peak1_idx in range(rleni):\n",
    "                mz = spec1_mz[peak1_idx]\n",
    "\n",
    "                low_bound = mz - tolerance\n",
    "                high_bound = mz + tolerance\n",
    "\n",
    "                for peak2_idx in range(lowest_idx, qlenj):\n",
    "                    mz2 = spec2_mz[peak2_idx] + shift\n",
    "                    if mz2 > high_bound:\n",
    "                        break\n",
    "                    if mz2 < low_bound:\n",
    "                        lowest_idx = peak2_idx\n",
    "                    else:\n",
    "                        if num_match < MATCH_LIMIT:\n",
    "                            matches[0, num_match] = peak1_idx\n",
    "                            matches[1, num_match] = peak2_idx\n",
    "                            num_match += 1\n",
    "                        else:\n",
    "                            overflow[i, j, 0] = 1  # This is the errorcode for overflow\n",
    "                            break\n",
    "\n",
    "            if num_match == 0:\n",
    "                return\n",
    "\n",
    "            # SLOW, calculate norm ( This should be done in several threads )\n",
    "            # score_norm = types.float32(0.0)\n",
    "            score_norm = types.float32(1.0)\n",
    "            score_norm_spec1 = types.float32(0.0)\n",
    "            score_norm_spec2 = types.float32(0.0)\n",
    "\n",
    "            for peak1_idx in range(rleni):\n",
    "                score_norm_spec1 += (\n",
    "                    (spec1_mz[peak1_idx] ** mz_power)\n",
    "                    * (spec1_int[peak1_idx] ** int_power)\n",
    "                ) ** 2\n",
    "            for peak2_idx in range(qlenj):\n",
    "                score_norm_spec2 += (\n",
    "                    (spec2_mz[peak2_idx] ** mz_power)\n",
    "                    * (spec2_int[peak2_idx] ** int_power)\n",
    "                ) ** 2\n",
    "            score_norm = math.sqrt(score_norm_spec1 * score_norm_spec2)\n",
    "\n",
    "            # Quite slow - Bubble sort (This should also be done in several threads)\n",
    "            # We need two cases, bubble sort up to 50 elems is fine\n",
    "            score = types.float32(0.0)\n",
    "            used_matches = types.int32(0)\n",
    "            for _ in range(0, num_match):\n",
    "                max_prod = types.float64(-1.0)\n",
    "                max_peak1_idx = -1\n",
    "                max_peak2_idx = -1\n",
    "\n",
    "                for sj in range(0, num_match):\n",
    "                    if matches[0, sj] >= 0:\n",
    "                        peak1_idx = matches[0, sj]\n",
    "                        peak2_idx = matches[1, sj]\n",
    "\n",
    "                        power_prod_spec1 = (spec1_mz[peak1_idx] ** mz_power) * (\n",
    "                            spec1_int[peak1_idx] ** int_power\n",
    "                        )\n",
    "                        power_prod_spec2 = (spec2_mz[peak2_idx] ** mz_power) * (\n",
    "                            spec2_int[peak2_idx] ** int_power\n",
    "                        )\n",
    "                        prod = power_prod_spec1 * power_prod_spec2\n",
    "                        if prod > max_prod:\n",
    "                            max_prod = prod\n",
    "                            max_peak1_idx = peak1_idx\n",
    "                            max_peak2_idx = peak2_idx\n",
    "\n",
    "                if max_prod > 0:\n",
    "                    for sj in range(0, num_match):\n",
    "                        if (\n",
    "                            matches[0, sj] == max_peak1_idx\n",
    "                            or matches[1, sj] == max_peak2_idx\n",
    "                        ):\n",
    "                            matches[0, sj] = -1  # \"Remove\" it\n",
    "                            matches[1, sj] = -1  # \"Remove\" it\n",
    "                    score += max_prod\n",
    "                    used_matches += 1\n",
    "\n",
    "                if max_prod < 0:\n",
    "                    break\n",
    "\n",
    "            score = score / score_norm\n",
    "\n",
    "            out[i, j, 0] = score\n",
    "            out[i, j, 1] = used_matches\n",
    "\n",
    "    def kernel(\n",
    "        rspec_cu: DeviceNDArray,\n",
    "        qspec_cu: DeviceNDArray,\n",
    "        lens_cu: DeviceNDArray,\n",
    "        out_cu: DeviceNDArray,\n",
    "        overflow_cu: DeviceNDArray,\n",
    "        stream: cuda.stream,\n",
    "    ):\n",
    "        _kernel[BLOCKS_PER_GRID, THREADS_PER_BLOCK, stream](\n",
    "            rspec_cu,\n",
    "            qspec_cu,\n",
    "            lens_cu,\n",
    "            out_cu,\n",
    "            overflow_cu,\n",
    "        )\n",
    "\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from matchms import Spectrum\n",
    "from matchms.filtering import (\n",
    "    add_losses,\n",
    "    normalize_intensities,\n",
    "    reduce_to_number_of_peaks,\n",
    "    require_minimum_number_of_peaks,\n",
    "    select_by_mz,\n",
    "    select_by_relative_intensity,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def spectra_peaks_to_tensor(\n",
    "    spectra: list, dtype: str = \"float32\"\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Working with GPU requires us to have a fixed shape for mz/int arrays.\n",
    "    This isn't the case for real-life data, so we have to \"pad\" the mz/int arrays.\n",
    "    We keep the real size of the mz/int in separate array, \"batch\". The regions out\n",
    "    of what \"batch\" specifies is undefined.\n",
    "\n",
    "    Returns:\n",
    "        spectra: [2, len(spectra)] float32\n",
    "        batch: [len(spectra)] int32\n",
    "    \"\"\"\n",
    "    sp_max_shape = max(len(s.peaks) for s in spectra)\n",
    "    mz = np.empty((len(spectra), sp_max_shape), dtype=dtype)\n",
    "    int = np.empty((len(spectra), sp_max_shape), dtype=dtype)\n",
    "    batch = np.empty(len(spectra), dtype=np.int32)\n",
    "    for i, s in enumerate(spectra):\n",
    "        # .to_numpy creates an unneeded copy - we don't need to do that twice\n",
    "        mz[i, : len(s.peaks)] = s._peaks.mz\n",
    "        int[i, : len(s.peaks)] = s._peaks.intensities\n",
    "        batch[i] = len(s.peaks)\n",
    "    spec = np.stack([mz, int], axis=0)\n",
    "    return spec, batch\n",
    "\n",
    "\n",
    "def get_ref_spectra_from_df(spectra_df, limit=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function will take a dataframe with spectra and return a list of matchms spectra.\n",
    "    Since all rows are independent, this function does this preprocessing in parallel (CPU).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # for index, row in spectra_df.iterrows():\n",
    "    def fn(index, row):\n",
    "        pbid = row[\"pbid\"]\n",
    "        precursor_mz = row[\"precursor_mz\"]\n",
    "        smiles = row[\"pb_smiles\"]\n",
    "        inchikey = row[\"pb_inchikey\"]\n",
    "        mz_array = np.array(json.loads(row[\"peaks_mz\"]))\n",
    "        intensity_array = np.array(json.loads(row[\"peaks_intensities\"]))\n",
    "        sp = Spectrum(\n",
    "            mz=mz_array,\n",
    "            intensities=intensity_array,\n",
    "            metadata={\n",
    "                \"id\": pbid,\n",
    "                \"precursor_mz\": precursor_mz,\n",
    "                \"smiles\": smiles,\n",
    "                \"inchikey\": inchikey,\n",
    "            },\n",
    "        )\n",
    "        sp = process_spectrum(sp)\n",
    "        return sp\n",
    "\n",
    "    if limit is not None:\n",
    "        spectra_df = spectra_df.head(limit)\n",
    "    spectra = Parallel(-2)(\n",
    "        delayed(fn)(index, row)\n",
    "        for index, row in tqdm(spectra_df.iterrows(), total=len(spectra_df))\n",
    "    )\n",
    "    spectra = [s for s in spectra if s is not None]\n",
    "    return spectra\n",
    "\n",
    "\n",
    "def process_spectrum(spectrum: np.ndarray) -> np.ndarray:\n",
    "    spectrum = select_by_mz(spectrum, mz_from=10.0, mz_to=1000.0)\n",
    "    spectrum = normalize_intensities(spectrum)\n",
    "    spectrum = select_by_relative_intensity(spectrum, intensity_from=0.001)\n",
    "    spectrum = reduce_to_number_of_peaks(spectrum, n_max=1000)\n",
    "    spectrum = require_minimum_number_of_peaks(spectrum, n_required=5)\n",
    "    return spectrum\n",
    "\n",
    "\n",
    "def batches(lst, batch_size):\n",
    "    \"\"\"\n",
    "    Batch data from the iterable into tuples of length n. The last batch may be shorter than n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]\n",
    "\n",
    "\n",
    "def mkdir(p: Path) -> Path:\n",
    "    p = Path(p)\n",
    "    p.mkdir(exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def argbatch(lst, batch_size):\n",
    "    \"\"\"\n",
    "    Batch data from the iterable into tuples of start-end indices\n",
    "    \"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield i, i + batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from itertools import product\n",
    "from multiprocessing import shared_memory\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import cuda\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "tolerance: float = 0.1\n",
    "shift: float = 0\n",
    "mz_power: float = 0\n",
    "int_power: float = 1\n",
    "\n",
    "## How many pairs per batch. Has to be a power of 2.\n",
    "# Hardware specific - An RTX2070 works best at around 1024 * 2\n",
    "# But Colab T4 GPU might work best at 1024 * 4\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# MATCH_LIMIT specifies max how many mz-mz pairs we could consider for each RQ pair, before we sort and filter.\n",
    "# E.g. a value of 256 usually causes around ~0.003% of RQ pairs to \"overflow\".\n",
    "# The overflown RQ scores will be strictly less than or equal to perfectly accurate score.\n",
    "# The mean absolute difference at 256, for all overflown pairs is on the order of ~1e-3\n",
    "# Small values of MATCH_LIMIT (e.g. 128, 64,) cause a dramatic speedup in the processing speed.\n",
    "MATCH_LIMIT = 256\n",
    "\n",
    "## GPU-specific constants\n",
    "THREADS_PER_BLOCK = (32, 32)\n",
    "BLOCKS_PER_GRID_X = math.ceil(BATCH_SIZE / THREADS_PER_BLOCK[0])\n",
    "BLOCKS_PER_GRID_Y = math.ceil(BATCH_SIZE / THREADS_PER_BLOCK[1])\n",
    "BLOCKS_PER_GRID = (BLOCKS_PER_GRID_X, BLOCKS_PER_GRID_Y)\n",
    "\n",
    "# Since Greedy cosine is an unstable algorithm, because approximate mz-mz values do not\n",
    "# result in approximately the same scores and number of matches.\n",
    "# So we need to use fp64 to minimize the deviation as much as possible.\n",
    "# Using float32 causes a significant speedup in the processing speed.\n",
    "dtype = \"float64\"\n",
    "\n",
    "# Data path\n",
    "reference_csv_file = \"data/input/example_dataset_tornike.csv\"\n",
    "query_csv_file = \"data/input/example_dataset_tornike.csv\"\n",
    "output_dir = \"data/output/\"\n",
    "\n",
    "# Limits\n",
    "# We consider only first LIMIT number of entries in CSVs\n",
    "LIMIT = 2048\n",
    "\n",
    "# For keeping track of experiments\n",
    "CONFIG = dict(\n",
    "    tolerance=tolerance,\n",
    "    shift=shift,\n",
    "    mz_power=mz_power,\n",
    "    int_power=int_power,\n",
    "    match_limit=MATCH_LIMIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    limit=LIMIT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2048/2048 [00:02<00:00, 691.79it/s] \n",
      "100%|██████████| 2048/2048 [00:00<00:00, 4988.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100001 references and 100001 queries\n"
     ]
    }
   ],
   "source": [
    "# We load CSV files using multiple threads\n",
    "ref_spectra_df_path = Path(reference_csv_file)\n",
    "ref_spectra_df = pd.read_csv(ref_spectra_df_path)\n",
    "references = get_ref_spectra_from_df(ref_spectra_df, limit=LIMIT)\n",
    "\n",
    "query_spectra_df_path = Path(query_csv_file)\n",
    "query_spectra_df = pd.read_csv(query_spectra_df_path)\n",
    "queries = get_ref_spectra_from_df(query_spectra_df, limit=LIMIT)\n",
    "\n",
    "print(f\"We have {len(ref_spectra_df)} references and {len(query_spectra_df)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 2070 with Max-Q Design'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-f6e241c8-f0ad-720e-be22-2713a6b0868d\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    }
   ],
   "source": [
    "# Numba Just-in-time compiles our kernel and bakes in our constants for performance.\n",
    "kernel = compile(\n",
    "    tolerance=tolerance,\n",
    "    shift=shift,\n",
    "    mz_power=mz_power,\n",
    "    int_power=int_power,\n",
    "    match_limit=MATCH_LIMIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches:  4\n",
      "Total pairs considered: 1993 * 1993 = 3972049\n",
      "Since 1993 isn't divisible by BATCH_SIZE, last batch will have 969 empty ROWS at the end\n",
      "Since 1993 isn't divisible by BATCH_SIZE, last batch will have 969 empty COLUMNS at the end\n"
     ]
    }
   ],
   "source": [
    "output_dir = mkdir(output_dir)\n",
    "\n",
    "TOTAL_BATCHES_X = math.ceil(len(references) / BATCH_SIZE)\n",
    "TOTAL_BATCHES_Y = math.ceil(len(queries) / BATCH_SIZE)\n",
    "TOTAL_BATCHES = TOTAL_BATCHES_X * TOTAL_BATCHES_Y\n",
    "print(\"Total batches: \", TOTAL_BATCHES)\n",
    "print(\n",
    "    f\"Total pairs considered: {len(references)} * {len(queries)} = {len(references) * len(queries)}\"\n",
    ")\n",
    "\n",
    "if len(references) % BATCH_SIZE != 0:\n",
    "    print(\n",
    "        f\"Since {len(references)} isn't divisible by BATCH_SIZE, last batch will have {len(references) % BATCH_SIZE} empty ROWS at the end\"\n",
    "    )\n",
    "if len(queries) % BATCH_SIZE != 0:\n",
    "    print(\n",
    "        f\"Since {len(queries)} isn't divisible by BATCH_SIZE, last batch will have {len(queries) % BATCH_SIZE} empty COLUMNS at the end\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 2it [00:00, 20.05it/s]\n",
      "Batch all queries: 2it [00:00, 21.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load each batch in memory so that we don't have to load any R,Q twice\n",
    "batches_r = []\n",
    "for rbatch in tqdm(batches(references, BATCH_SIZE), desc=\"Batch all references\"):\n",
    "    rspec, rlen = spectra_peaks_to_tensor(rbatch, dtype=dtype)\n",
    "    batches_r.append([rspec, rlen])\n",
    "\n",
    "batches_q = list()\n",
    "for qbatch in tqdm(batches(queries, BATCH_SIZE), desc=\"Batch all queries\"):\n",
    "    qspec, qlen = spectra_peaks_to_tensor(qbatch, dtype=dtype)\n",
    "    batches_q.append([qspec, qlen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 2it [00:00, 29.69it/s]\n",
      "Batch all queries: 2it [00:00, 28.42it/s]\n"
     ]
    }
   ],
   "source": [
    "streams = [cuda.stream() for _ in range(TOTAL_BATCHES)]\n",
    "\n",
    "batches_r = []\n",
    "for bstart, bend in tqdm(argbatch(references, BATCH_SIZE), desc=\"Batch all references\"):\n",
    "    rbatch = references[bstart:bend]\n",
    "    rspec, rlen = spectra_peaks_to_tensor(rbatch, dtype=dtype)\n",
    "    batches_r.append([rspec, rlen, bstart, bend])\n",
    "\n",
    "batches_q = list()\n",
    "for bstart, bend in tqdm(argbatch(queries, BATCH_SIZE), desc=\"Batch all queries\"):\n",
    "    qbatch = queries[bstart:bend]\n",
    "    qspec, qlen = spectra_peaks_to_tensor(qbatch, dtype=dtype)\n",
    "    batches_q.append([qspec, qlen, bstart, bend])\n",
    "\n",
    "batches_rq = list(product(batches_r, batches_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed at 2574696.6 pairs/sec\n",
      "Estimated 16.18hrs per 100k x 1.5mln\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "! rm -rf data/output/*\n",
    "\n",
    "start = perf_counter()\n",
    "# We initialize a pool of 3 workers that will offload results to disk\n",
    "with ThreadPool(3) as pool:\n",
    "    # We loop over all batchs in sequence\n",
    "    for batch_i in tqdm(range(TOTAL_BATCHES)):\n",
    "\n",
    "        # Each batch has own CUDA stream so that the GPU is as busy as possible\n",
    "        stream = streams[batch_i]\n",
    "\n",
    "        # Shared memory allows pool workers to read array without copying it\n",
    "        out_shm = shared_memory.SharedMemory(\n",
    "            create=True, size=(BATCH_SIZE * BATCH_SIZE * 2 * 4)\n",
    "        )\n",
    "        out = np.ndarray(\n",
    "            shape=(BATCH_SIZE, BATCH_SIZE, 2), dtype=\"float32\", buffer=out_shm.buf\n",
    "        )\n",
    "        overflow_shm = shared_memory.SharedMemory(\n",
    "            create=True, size=(BATCH_SIZE * BATCH_SIZE * 1 * 1)\n",
    "        )\n",
    "        overflow = np.ndarray(\n",
    "            shape=(BATCH_SIZE, BATCH_SIZE, 1), dtype=\"uint8\", buffer=overflow_shm.buf\n",
    "        )\n",
    "\n",
    "        # We order empty space for results on GPU RAM\n",
    "        out_cu = cuda.device_array(\n",
    "            (BATCH_SIZE, BATCH_SIZE, 2), dtype=\"float32\", stream=stream\n",
    "        )\n",
    "        overflow_cu = cuda.device_array(\n",
    "            (BATCH_SIZE, BATCH_SIZE, 1), dtype=\"uint8\", stream=stream\n",
    "        )\n",
    "\n",
    "        # We get our batch and lengths (lengths are different for different spectra)\n",
    "        (rspec, rlen, rstart, rend), (qspec, qlen, qstart, qend) = batches_rq[batch_i]\n",
    "        lens = np.zeros((2, BATCH_SIZE), \"int32\")\n",
    "        lens[0, : len(rlen)] = rlen\n",
    "        lens[1, : len(qlen)] = qlen\n",
    "\n",
    "        # We make sure main resources remain on CPU RAM\n",
    "        with cuda.pinned(\n",
    "            rspec,\n",
    "            qspec,\n",
    "            lens,\n",
    "            out,\n",
    "            overflow,\n",
    "        ):\n",
    "\n",
    "            # We order the stream to copy input data to GPU RAM\n",
    "            rspec_cu = cuda.to_device(rspec, stream=stream)\n",
    "            qspec_cu = cuda.to_device(qspec, stream=stream)\n",
    "            lens_cu = cuda.to_device(lens, stream=stream)\n",
    "\n",
    "            # We order the stream to execute kernel (this is scheduled, it will execute, but we can't force it)\n",
    "            kernel(rspec_cu, qspec_cu, lens_cu, out_cu, overflow_cu, stream=stream)\n",
    "\n",
    "            # We order a data return\n",
    "            out_cu.copy_to_host(out, stream=stream)\n",
    "            overflow_cu.copy_to_host(overflow, stream=stream)\n",
    "\n",
    "            # We create a function that will execute when this stream is done working\n",
    "            # It is important to be quick here - so main work of writing to disk\n",
    "            # Is handled by pool workers, not callback stream.\n",
    "            def end_of_stream_callback(*args):\n",
    "                def thread_worker(name1, name2):\n",
    "                    ex_shm = shared_memory.SharedMemory(name=name1)\n",
    "                    out = np.ndarray(\n",
    "                        shape=(BATCH_SIZE, BATCH_SIZE, 2),\n",
    "                        dtype=np.float32,\n",
    "                        buffer=ex_shm.buf,\n",
    "                    )\n",
    "                    np.save(\n",
    "                        f\"data/output/{rstart}-{rend}.{qstart}-{qend}.score.npy\", out\n",
    "                    )\n",
    "\n",
    "                    ex_shm.unlink()\n",
    "                    ex_shm = shared_memory.SharedMemory(name=name2)\n",
    "                    overflow = np.ndarray(\n",
    "                        shape=(BATCH_SIZE, BATCH_SIZE, 1),\n",
    "                        dtype=np.uint8,\n",
    "                        buffer=ex_shm.buf,\n",
    "                    )\n",
    "                    np.save(\n",
    "                        f\"data/output/{rstart}-{rend}.{qstart}-{qend}.ovfl.npy\",\n",
    "                        overflow,\n",
    "                    )\n",
    "                    ex_shm.unlink()\n",
    "\n",
    "                pool.apply_async(\n",
    "                    thread_worker,\n",
    "                    args=[out_shm.name, overflow_shm.name],\n",
    "                    error_callback=lambda e: print(\"Thread error\", e),\n",
    "                )\n",
    "\n",
    "            stream.add_callback(\n",
    "                callback=end_of_stream_callback,\n",
    "            )\n",
    "\n",
    "# We wait for all streams to finish their work everywhere\n",
    "cuda.synchronize()\n",
    "\n",
    "# We can now calculate our performance fairly\n",
    "duration = perf_counter() - start\n",
    "persec = len(references) * len(queries) / duration\n",
    "print(f\"Speed at {persec:.1f} pairs/sec\")\n",
    "print(f\"Estimated {(100_000 * 1_500_000 / persec) / 3600:.2f}hrs per 100k x 1.5mln\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query RQ pairs with condition on score\n",
    "\n",
    "This is still TODO on large outputs, since filtering gigabytes worth of numpy arrays will take forever. For now, CPU implementation should suffice - or we could integrate this \"filtering\" behaviour directly into Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-1024.0-1024.score\n",
      "0-1024.1024-2048.score\n",
      "1024-2048.0-1024.score\n",
      "1024-2048.1024-2048.score\n",
      "Reference       UInt32\n",
      "Query           UInt32\n",
      "Score          Float32\n",
      "Num_Matches     UInt16\n",
      "dtype: object Memory  1.079728 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Query</th>\n",
       "      <th>Score</th>\n",
       "      <th>Num_Matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990495</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.977393</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.934253</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.877143</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15022</th>\n",
       "      <td>1992</td>\n",
       "      <td>1273</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15023</th>\n",
       "      <td>1992</td>\n",
       "      <td>1609</td>\n",
       "      <td>0.761673</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15024</th>\n",
       "      <td>1992</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.802216</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15025</th>\n",
       "      <td>1992</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.945183</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15026</th>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41528 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Reference  Query     Score  Num_Matches\n",
       "0              0      0       1.0           14\n",
       "1              0      1  0.990495           14\n",
       "2              0      2  0.977393           11\n",
       "3              0      3  0.934253           11\n",
       "4              0      4  0.877143           11\n",
       "...          ...    ...       ...          ...\n",
       "15022       1992   1273    0.8502           39\n",
       "15023       1992   1609  0.761673           33\n",
       "15024       1992   1990  0.802216           37\n",
       "15025       1992   1991  0.945183           45\n",
       "15026       1992   1992       1.0           48\n",
       "\n",
       "[41528 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_score = 0.75  # Min score\n",
    "results = pd.DataFrame([], columns=[\"Reference\", \"Query\", \"Score\", \"Num_Matches\"])\n",
    "\n",
    "score_files = sorted(Path(output_dir).glob(\"*.score.npy\"))\n",
    "for score_file in score_files:\n",
    "    print(score_file.stem)\n",
    "    match = re.match(r\"(\\d+)-(\\d+)\\.(\\d+)-(\\d+)\", score_file.stem)\n",
    "    rstart, rend, qstart, qend = map(int, match.groups())\n",
    "    score = np.load(score_file)\n",
    "\n",
    "    # Condition query\n",
    "    pairs_relative = np.argwhere(score[..., 0] >= min_score)\n",
    "    # We have to pad pairs with their actual locations on full grid\n",
    "    pairs_absolute = pairs_relative + [rstart, qstart]\n",
    "\n",
    "    # score, num_matches = get_one_specific(ref_idx, que_idx)\n",
    "    r, q = pairs_relative.T\n",
    "    score, num_match = score[r, q].T\n",
    "\n",
    "    r, q = pairs_absolute.T\n",
    "    result = pd.DataFrame(\n",
    "        dict(\n",
    "            Reference=r.astype(\"uint32\"),\n",
    "            Query=q.astype(\"uint32\"),\n",
    "            Score=score.astype(\"float32\"),\n",
    "            Num_Matches=num_match.astype(\"uint16\"),\n",
    "        )\n",
    "    ).convert_dtypes()\n",
    "    results = pd.concat([results, result], axis=0, copy=False)\n",
    "\n",
    "print(results.dtypes, \"Memory \", results.memory_usage().sum() / 1e6, \"MB\")\n",
    "\n",
    "assert (result.Score >= min_score).all(), \"Something wrong with filtering!\"\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with how large the `results` dataframe can get! You might run our of RAM before it's all loaded into memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
